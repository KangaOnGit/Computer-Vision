{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1151625,"sourceType":"datasetVersion","datasetId":650614}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglehub","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Download Dataset\ndata_dir = kagglehub.dataset_download('andrewmvd/dog-and-cat-detection')\nprint(f\"Path to Dataset: {data_dir}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Libraries**","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nfrom PIL import Image\nfrom torchvision import transforms, models\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom torchvision.models.resnet import ResNet18_Weights\n\n# Data Visualizer\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Data Reader\nimport xml.etree.ElementTree as ET\n\nimport time","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"# **Sample Data**","metadata":{}},{"cell_type":"code","source":"sample_annotation = '/kaggle/input/dog-and-cat-detection/annotations/Cats_Test0.xml'\nsample_tree = ET.parse(sample_annotation)\nsample_root = sample_tree.getroot()\n\nfor element in sample_root:\n    print(f\"{element} is an element in {sample_annotation}\")\n\nfolder_tag = sample_root.findall('folder')\nfilename_tag = sample_root.findall('filename')\nsize_tag = sample_root.findall('size')\nsegmented_tag = sample_root.findall('segmented')\nobject_tag = sample_root.findall('object')\nprint(\"-\"*59)\nprint(f\"Type of folder_tag is: {type(folder_tag)}\")\nprint(f\"Type of filename_tag is: {type(filename_tag)}\")\nprint(f\"Type of size_tag is: {type(size_tag)}\")\nprint(f\"Type of segmented_tag is: {type(segmented_tag)}\")\nprint(f\"Type of object_tag is: {type(object_tag)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for element in folder_tag[0]:\n    print(f\"{element} is an element in folder_tag\")\nprint(\"-\"*59)\n\nfor element in filename_tag[0]:\n    print(f\"{element} is an element in filename_tag\")\nprint(\"-\"*59)\n\nfor element in size_tag[0]:\n    print(f\"{element} is an element in size_tag\")\nprint(\"-\"*59)\n\nfor i in segmented_tag[0]:\n    print(f\"{element} is an element in segmented_tag\")\nprint(\"-\"*59)\n\nfor element in object_tag[0]:\n    print(f\"{element} is an element in object_tag\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for element in object_tag[0][5]:\n    print(f\"{element} is an element in object_tag\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(object_tag[0][0].text)\nprint(type(object_tag[0][0].text))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tree Structure\n```\n<annotation>\n    <folder>images</folder>\n    <filename>Cats_Test0.png</filename>\n    <size>\n        <width>233</width>\n        <height>350</height>\n        <depth>3</depth>\n    </size>\n    <segmented>0</segmented>\n    <object>\n        <name>cat</name>\n        <pose>Unspecified</pose>\n        <truncated>0</truncated>\n        <occluded>0</occluded>\n        <difficult>0</difficult>\n        <bndbox>\n            <xmin>83</xmin>\n            <ymin>29</ymin>\n            <xmax>197</xmax>\n            <ymax>142</ymax>\n        </bndbox>\n    </object>\n</annotation>","metadata":{}},{"cell_type":"markdown","source":"----","metadata":{}},{"cell_type":"markdown","source":"# **Dataset**","metadata":{}},{"cell_type":"code","source":"class ImageDataset(Dataset):\n    def __init__(self, annotations_dir, image_dir, transform = None):\n        self.annotations_dir = annotations_dir\n        self.image_dir = image_dir\n        self.transform = transform\n        self.image_files = self.filter_images_with_multiple_objects()\n\n    def filter_images_with_multiple_objects(self):\n        valid_image_files = []\n\n        # Loop through all Image in directory\n        for image in sorted(os.listdir(self.image_dir)):\n            \n            # Check if annotation exists\n            if os.path.isfile(os.path.join(self.image_dir, image)):\n                annotation_name = os.path.splitext(image)[0] + '.xml'\n                annotation_path = os.path.join(self.annotations_dir, annotation_name)\n                try:\n                    tree = ET.parse(annotation_path)\n                    root = tree.getroot()\n                    object_tag = root.findall('object')\n                    if len(object_tag) <= 1:\n                        valid_image_files.append(image)\n                    else:\n                        print(f\"Image {image} has multiple objects and will be excluded from the dataset\")\n                        print(\" \")\n                except FileNotFoundError:\n                    valid_image_files.append(image)\n        return valid_image_files\n                    \n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        # Image Path\n        img_name = self.image_files[idx]\n        img_path = os.path.join(self.image_dir, img_name)\n\n        # Load Image\n        image = Image.open(img_path).convert(\"RGB\")\n\n        # Annotation Path\n        annotation_name = os.path.splitext(img_name)[0] + '.xml'\n        annotation_path = os.path.join(self.annotations_dir, annotation_name)\n        try:\n            # Parse Annotation\n            tree = ET.parse(annotation_path)\n            root = tree.getroot()\n            label_name = None\n    \n            object_tag = root.findall('object')\n            # We are working with 1 object/image so no need for loop\n            name = object_tag[0][0].text\n            if (label_name is None):\n                label_name = name\n            label = 0 if label_name == 'cat' else 1 if label_name == 'dog' else -1\n        except FileNotFoundError:\n            print(f\"Annotation file {annotation_name} missing. Assining label as -1\")\n            label = -1\n        \n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Directories\nannotations_dir = os.path.join(data_dir, 'annotations')\nimage_dir = os.path.join(data_dir, 'images')\n\n# Get list of image files and create dummy dataframe to split the data\nimage_files = [image for image in sorted(os.listdir(image_dir)) if os.path.isfile(os.path.join(image_dir, image))]\ndf = pd.DataFrame({'image_name': image_files})\nprint(f\"Dummy Dataset:\")\nprint(df.head())\nprint(\"-\"*59)\n\n# Split data\ntrain_df, val_df = train_test_split(df, test_size = .2, random_state = 42)\nprint(f\"Train Dataset:\")\nprint(train_df.head())\nprint(\"-\"*59)\nprint(f\"Validation Dataset:\")\nprint(val_df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Augmentation/Transforms\n\ntransform = transforms.Compose(\n    [\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(\n            mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]\n        )\n    ]\n)\n\n# Datasets\ntrain_set = ImageDataset(annotations_dir, image_dir, transform = transform)\nval_set = ImageDataset(annotations_dir, image_dir, transform = transform)\n\n# Filter datasets based on train_df and val_df\n# Check if image in image_files is in train_df \ntrain_set.image_files = [file for file in train_set.image_files\n                         if file in train_df['image_name'].values]\nval_set.image_files = [file for file in val_set.image_files \n                       if file in val_df['image_name'].values]\n\n# Dataloaders\ntrain_loader = DataLoader(train_set, batch_size = 32, shuffle = True)\nval_loader = DataLoader(val_set, batch_size = 32, shuffle = False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Model**","metadata":{}},{"cell_type":"code","source":"# Model\nmodel = models.resnet18(weights = ResNet18_Weights.DEFAULT)\nnum_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, 2) # 2 Classes: Cat n' Dog\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# Loss n' Optimizer\ncriterion = nn.CrossEntropyLoss()\noptim = optim.Adam(model.parameters(), lr = 0.001)\n\nprint(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Training**","metadata":{}},{"cell_type":"code","source":"def generate_images(model, inputs, labels):\n    model.eval()\n    class_names = ['cat', 'dog']\n    with torch.no_grad():\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)  # Get predicted class\n\n    # Move everything back to CPU for visualization\n    inputs, labels, preds = inputs.cpu(), labels.cpu(), preds.cpu()\n\n    # Get the first (and only) sample\n    img = inputs[0].numpy().transpose((1, 2, 0))  # Convert to HWC format\n    label = labels[0].item()\n    pred = preds[0].item()\n\n    # Check if prediction is correct\n    correct = pred == label\n    color = \"purple\" if correct else \"red\"\n\n    # Reverse normalization (assuming ImageNet mean & std)\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    img = std * img + mean  # Undo normalization\n    img = np.clip(img, 0, 1)  # Ensure valid pixel values\n\n    # Plot the image with prediction\n    plt.figure(figsize=(5, 5))\n    plt.imshow(img)\n    plt.axis(\"off\")\n    plt.title(f\"Pred: {class_names[pred]} | Real: {class_names[label]}\", color=color, fontsize=12)\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def eval_epoch(model, criterion, val_set, device):\n    model.eval()\n    corr = 0\n    total = 0\n    losses = []\n    with torch.no_grad():\n        for idx, (inputs, labels) in enumerate(val_set):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            \n            loss = criterion(outputs, labels)\n            losses.append(loss.item())\n            _, outputs = outputs.max(1)\n            corr += (outputs == labels).sum()\n            total += inputs.size(0)\n    epoch_loss = sum(losses)/len(losses)\n    epoch_accuracy = corr/total\n    return epoch_accuracy, epoch_loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_epoch(model, optim, criterion, train_set, device, epoch = 0, log_interval = 10):\n    model.train()\n    total = 0\n    corr = 0\n    losses = []\n    start_time = time.time()\n\n    for idx, (inputs, labels) in enumerate(train_set):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        optim.zero_grad()\n\n        outputs = model(inputs)\n\n        loss = criterion(outputs, labels)\n        losses.append(loss.item())\n\n        loss.backward()\n        optim.step()\n        \n        _, outputs = outputs.max(1)\n        total += inputs.size(0)\n        corr += (outputs == labels).sum()\n\n        if idx % log_interval == 0 and idx > 0:\n            elapsed = time.time() - start_time\n\n            print(\n                f\"| Epoch {epoch} | {idx}/{len(train_set)} Batches | {corr/total} Accuracy\"\n            )\n            total, corr = 0, 0\n            start_time = time.time()\n    epoch_accuracy = corr/total\n    epoch_loss = sum(losses)/len(losses)\n    return epoch_accuracy, epoch_loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(model, model_name, save_model, optim, criterion,\n         train_set, val_set, num_epochs, device):\n    train_accuracy, train_losses = [], []\n    eval_accuracy, eval_losses = [], []\n    best_accuracy_eval = -1\n    times = []\n    for epoch in range(1, num_epochs + 1):\n        print(f\"Starting Epoch {epoch}....\")\n        print(\" \")\n        epoch_start_time = time.time()\n\n        # Train\n        train_acc, train_loss = train_epoch(model, optim, criterion, train_set, device, epoch)\n        train_accuracy.append(train_acc)\n        train_losses.append(train_loss)\n\n        # Evaluate\n        eval_acc, eval_loss = eval_epoch(model, criterion, val_set, device)\n        eval_accuracy.append(eval_acc)\n        eval_losses.append(eval_loss)\n\n        # Save best model\n        if best_accuracy_eval < eval_acc:\n            torch.save(model.state_dict(), save_model + f\"/{model_name}.pt\")\n            inputs_t, targets_t = next(iter(val_set))\n            print(\" \")\n            generate_images(model, inputs_t, targets_t)\n            print(\" \")\n            best_accuracy_eval = eval_acc\n        times.append(time.time() - epoch_start_time)\n\n        print(\"-\"*59)\n        print(\n            f\"| End of Epoch {epoch} | Time Taken: {time.time() - epoch_start_time} | Train Accuracy: {train_acc} | Train loss: {train_loss}, | Val Accuracy: {eval_acc} | Val Loss: {eval_loss}\"\n        )\n        print(\" \")\n        print(f\"Epoch {epoch} Ending...\")\n        print(\"-\" *59)\n    # Load best model\n    model.load_state_dict(torch.load(save_model + f'/{model_name}.pt'))\n    model.eval()\n    metrics = {\n        'Train Accuracy': train_accuracy,\n        'Train Loss': train_losses,\n        'Eval Accuracy': eval_accuracy,\n        'Eval Loss': eval_losses,\n        'Time': times\n    }\n    return model, metrics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_result(num_epochs, train_accuracy, eval_accuracy, train_losses, eval_losses):\n    epochs = list(range(num_epochs))\n\n    # Function to ensure tensors are moved to CPU and converted to numpy\n    def ensure_cpu(tensor):\n        if isinstance(tensor, torch.Tensor):\n            if tensor.device != 'cpu':\n                print(f\"Tensor is on {tensor.device}. Moving to CPU.\")\n            return tensor.cpu().numpy()\n        return tensor\n\n    # Move tensors to CPU and convert to numpy if necessary\n    train_accuracy = ensure_cpu(train_accuracy)\n    eval_accuracy = ensure_cpu(eval_accuracy)\n    train_losses = ensure_cpu(train_losses)\n    eval_losses = ensure_cpu(eval_losses)\n\n    # Debugging: check the types and shapes of tensors\n    print(f\"train_accuracy: {train_accuracy.shape if isinstance(train_accuracy, np.ndarray) else type(train_accuracy)}\")\n    print(f\"eval_accuracy: {eval_accuracy.shape if isinstance(eval_accuracy, np.ndarray) else type(eval_accuracy)}\")\n    print(f\"train_losses: {train_losses.shape if isinstance(train_losses, np.ndarray) else type(train_losses)}\")\n    print(f\"eval_losses: {eval_losses.shape if isinstance(eval_losses, np.ndarray) else type(eval_losses)}\")\n\n    # Plotting\n    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n    \n    axs[0].plot(epochs, train_accuracy, label=\"Training\")\n    axs[0].plot(epochs, eval_accuracy, label=\"Evaluation\")\n    axs[0].set_title(\"Accuracy\")\n    axs[0].set_xlabel(\"Epoch\")\n    axs[0].set_ylabel(\"Accuracy\")\n    axs[0].legend()\n\n    axs[1].plot(epochs, train_losses, label=\"Training\")\n    axs[1].plot(epochs, eval_losses, label=\"Evaluation\")\n    axs[1].set_title(\"Loss\")\n    axs[1].set_xlabel(\"Epoch\")\n    axs[1].set_ylabel(\"Loss\")\n    axs[1].legend()\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Results**","metadata":{}},{"cell_type":"code","source":"epochs = 10\nsave_model = './ResNet18'\nos.makedirs(save_model, exist_ok = True)\n\nResNet18, metrics = train(\n    model, 'ResNet18Classification', save_model, optim, criterion, train_loader, val_loader, epochs, device\n)","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_predictions(model, dataloader, num_images=10):\n    model.eval()\n    images_so_far = 0\n    class_names = [\"cat\", \"dog\"]\n    fig = plt.figure(figsize=(10, 8))\n\n    with torch.no_grad():\n        for i, (images, labels) in enumerate(dataloader):\n            images = images.to(device)\n            labels = labels.to(device)\n\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n\n            for j in range(images.size()[0]):\n                images_so_far += 1\n                ax = plt.subplot(num_images // 2, 2, images_so_far)\n                ax.axis(\"off\")\n                ax.set_title(\n                    f\"Predicted: {class_names[preds[j]]}, Actual: {class_names[labels[j]]}\",\n                    fontdict={\n                        \"fontsize\": 10,\n                        \"color\": \"purple\" if preds[j] == labels[j] else \"red\",\n                    },\n                )\n                # Denormalize and display the image\n                img = images.cpu().data[j].numpy().transpose((1, 2, 0))\n                mean = [0.485, 0.456, 0.406]\n                std = [0.229, 0.224, 0.225]\n                img = std * img + mean\n                img = np.clip(img, 0, 1)\n                plt.imshow(img)\n\n                if images_so_far == num_images:\n                    plt.tight_layout()\n                    return\n\n\nvisualize_predictions(model, val_loader)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_confusion_matrix(model, dataloader):\n    model.eval()\n    y_true = []\n    y_pred = []\n\n    with torch.no_grad():\n        for images, labels in dataloader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(preds.cpu().numpy())\n\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['cat', 'dog'], yticklabels=['cat', 'dog'])\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(\"Confusion Matrix\")\n    plt.show()\n\nplot_confusion_matrix(model, val_loader)","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_result(\n    epochs,\n    metrics[\"Train Accuracy\"],\n    metrics[\"Eval Accuracy\"],\n    metrics[\"Train Loss\"],\n    metrics[\"Eval Loss\"]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}